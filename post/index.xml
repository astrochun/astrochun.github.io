<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Chun Ly</title>
    <link>https://astrochun.github.io/post/</link>
      <atom:link href="https://astrochun.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>MIT License</copyright><lastBuildDate>Sun, 29 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://astrochun.github.io/img/icon-192.png</url>
      <title>Posts</title>
      <link>https://astrochun.github.io/post/</link>
    </image>
    
    <item>
      <title>Building a streamlit app to serve UArizona salary data and deploying it on Heroku</title>
      <link>https://astrochun.github.io/post/salary_app/</link>
      <pubDate>Sun, 29 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/salary_app/</guid>
      <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.wildcat.arizona.edu/&#34;&gt;Daily Wildcat&lt;/a&gt; of the
&lt;a href=&#34;https://arizona.edu&#34;&gt;University of Arizona&lt;/a&gt; (UArizona) has routinely provided
the salary of UArizona employees for nearly a decade through public requests.
While this allowed for the easy viewing of information for an individual,
the analysis from the Daily Wildcat was rather limited. As such, I wanted to
build a data visualization app that would allow anyone to explore the data for
100% transparency. I call it &lt;code&gt;sapp4ua&lt;/code&gt;, which is short for the
salary app for UArizona. It&#39;s available at &lt;a href=&#34;https://sapp4ua.herokuapp.com&#34;&gt;https://sapp4ua.herokuapp.com&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;The salary data were available as machine-readable CSV files, so it made sense
to build a Python application that uses &lt;code&gt;pandas&lt;/code&gt; to extract, transform, and
load (ETL) the data. I heard about &lt;code&gt;streamlit&lt;/code&gt;, a Python web framework that
would make it faster to build such an app. Essentially &lt;code&gt;streamlit&lt;/code&gt; provides a
RESTful application programming interface (API) with a front-end layer that
interacts with Python scripts that do data analysis and visualization. It
took me about a weekend to understand &lt;code&gt;streamlit&lt;/code&gt; and its &amp;ldquo;widgets&amp;rdquo;, which
are interactive tools that could be used to perform ETL actions with the data.
From there, I put together the software to load the data and provide different
&amp;ldquo;data views&amp;rdquo; that were different interactions with the data. The code is
available on &lt;a href=&#34;https://www.github.com/astrochun/uarizona-salary-app&#34;&gt;GitHub&lt;/a&gt;
under an MIT License. There are many resources and blogs about building a
&lt;code&gt;streamlit&lt;/code&gt; app, so I won&#39;t go in details here, but I will illustrate some
aspects:&lt;/p&gt;
&lt;p&gt;Loading in data can be simple.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import pandas as pd
import streamlit as st

# List of fiscal years
FY_LIST = [
    &#39;FY2019-20&#39;, &#39;FY2018-19&#39;, &#39;FY2017-18&#39;, &#39;FY2016-17&#39;,
    &#39;FY2014-15&#39;, &#39;FY2013-14&#39;, &#39;FY2011-12&#39;,
]


@st.cache
def load_data():

    file_id = {
        &#39;FY2019-20&#39;: &#39;1d2l29_T-mOh05bglPlwAFlzeV1PIkRXd&#39;,
        &#39;FY2018-19&#39;: &#39;1paxrUyW1wZuK3bjSL_L7ckKEC6xslZJe&#39;,
        &#39;FY2017-18&#39;: &#39;1AnRaPpbRTLVyqdeqe6vkPMYgbNnw9zia&#39;,
        &#39;FY2016-17&#39;: &#39;1rXBuuXit5oWKtfnA05gsNtsWAyESeIs2&#39;,
        &#39;FY2014-15&#39;: &#39;1ZANVDr6Kw40MJYiOENWbLMTFEMWyf7f4&#39;,
        &#39;FY2013-14&#39;: &#39;1rQ8A2CIVhDYu0lESKVh72h6VUd8gIEFl&#39;,
        &#39;FY2011-12&#39;: &#39;1fQOzEHiOvc_H1NcLMlK3KVV1DJkRbRuX&#39;,
    }

    data_dict = {}
    for year in FY_LIST:
        data_dict[year] = pd.read_csv(
            f&#39;https://drive.google.com/uc?id={file_id[year]}&#39;
        )

    return data_dict


def main():

    # Load data
    data_dict = load_data()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code loads each previous year of available data into a &lt;code&gt;dict&lt;/code&gt;
of &lt;code&gt;pandas&lt;/code&gt; DataFrames. The &lt;code&gt;st.cache&lt;/code&gt; decorator ensures that the data is
loaded into memory only once. This ensures a shorter response time when it&#39;s
deployed and users are accessing the app frequently. Here, I made the data
publicly available in Google Drive. I could have gone with an SQL server,
but given the  small record sizes (~10,000 records), and the reliability
of Google Drive, this worked well for an MVP.&lt;/p&gt;
&lt;p&gt;To select from the data, I utilized &lt;code&gt;streamlit&lt;/code&gt; sidebar widgets to add the
layer for selection. For example, the following selects the data for one
year:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def select_fiscal_year() -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Sidebar widget to select fiscal year&amp;quot;&amp;quot;&amp;quot;

    st.sidebar.markdown(&#39;### Select fiscal year:&#39;)
    fy_select = st.sidebar.selectbox(&#39;&#39;, FY_LIST, index=0).split(&#39; &#39;)[0]

    return fy_select


def main():
    # Load data
    data_dict = load_data()

    fy_select = select_fiscal_year()

    # Select dataframe
    df = data_dict[fy_select]
    st.sidebar.text(f&amp;quot;{fy_select} data imported!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in something that looks like this:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot1.png&#34; data-caption=&#34;Sidebar tool selection for fiscal year&#34;&gt;
&lt;img src=&#34;screenshot1.png&#34; alt=&#34;&#34; width=&#34;40%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sidebar tool selection for fiscal year
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As discussed earlier, interactions with the data is done with different views.
Currently, I have six of them:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Trends: General facts and numbers (e.g. number of employees, salary budget),
for each fiscal year.&lt;/li&gt;
&lt;li&gt;Salary Summary: Statistics and percentile salary data, includes salary
histogram.&lt;/li&gt;
&lt;li&gt;Highest Earners: Extract data above a minimum salary.&lt;/li&gt;
&lt;li&gt;College/Division Data: Similar to Salary Summary but extracted for each
college(s)/division(s).&lt;/li&gt;
&lt;li&gt;Department Data: Similar to Salary Summary but extracted for each
department(s).&lt;/li&gt;
&lt;li&gt;Individual Search: Search for all data for individuals or by looking at
each department. You can read more &lt;a href=&#34;#whats-important-for-end-users&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This can easily be done on the sidebar and then the main page is loaded:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DATA_VIEWS = [
    &#39;About&#39;, &#39;Individual Search (NEW)&#39;, &#39;Trends&#39;, &#39;Salary Summary&#39;,
    &#39;Highest Earners&#39;, &#39;College/Division Data&#39;, &#39;Department Data&#39;,
]

def select_data_view() -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Sidebar widget to select your data view&amp;quot;&amp;quot;&amp;quot;

    st.sidebar.markdown(&#39;### Select your data view:&#39;)
    view_select = st.sidebar.selectbox(&#39;&#39;, DATA_VIEWS, index=0)

    return view_select


def main():
    # Sidebar, select data view
    view_select = select_data_view()

    if view_select == &#39;About&#39;:
        views.about_page()

    if view_select == &#39;Trends&#39;:
        views.trends_page(data_dict, pay_norm)

    if view_select == &#39;Salary Summary&#39;:
        views.salary_summary_page(df, pay_norm)

    if view_select == &#39;Highest Earners&#39;:
        views.highest_earners_page(df)

    # Select by College Name
    if view_select == &#39;College/Division Data&#39;:
        views.subset_select_data_page(df, COLLEGE_NAME, &#39;college&#39;,
                                      pay_norm)

    # Select by Department Name
    if view_select == &#39;Department Data&#39;:
        views.subset_select_data_page(df, &#39;Department&#39;, &#39;department&#39;,
                                      pay_norm)

    # Load individual search
    if view_select == &#39;Individual Search&#39;:
        views.individual_search_page(data_dict, unique_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;views&lt;/code&gt; is a Python module and each function displays
different content in the main page. Here&#39;s a screenshot for the
Salary Summary page:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot2.png&#34; data-caption=&#34;One of the data views from sapp4ua.&#34;&gt;
&lt;img src=&#34;screenshot2.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    One of the data views from &lt;code&gt;sapp4ua&lt;/code&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;deploying-it-on-heroku&#34;&gt;Deploying it on Heroku&lt;/h2&gt;
&lt;p&gt;With a functional app, it&#39;s easy to have it locally deployed with the
following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;streamlit run salary_app/main.py
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make it available on a production instance, I looked into hosting services
(called Platform as a Service), and it turned out that
&lt;a href=&#34;https://heroku.com/about&#34;&gt;Heroku&lt;/a&gt; has a free tier for serverless deployment.
It seems as though it&#39;s one of the popular options among &lt;code&gt;streamlit&lt;/code&gt; users to
deploy their apps. Note that &lt;code&gt;streamlit&lt;/code&gt; runs a web server so static
site hosting services (e.g., GitHub Pages) will not work. To get started,
first &lt;a href=&#34;https://signup.heroku.com/&#34;&gt;sign-up&lt;/a&gt; for a Heroku account. To create
your first Heroku project, select the &amp;ldquo;Create New App&amp;rdquo; option from the New
button. Here the project name needs to be unique across all of Heroku
because it will point to &lt;code&gt;https://&amp;lt;project-name&amp;gt;.herokuapp.com&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now you have a project that can be triggered to deploy after any codebase
changes. What I learned is that Heroku is developer friendly.
It has at least three ways to deploy it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Manually using &lt;code&gt;git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Using GitHub integration&lt;/li&gt;
&lt;li&gt;Using a GitHub Action&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;commandline-deployment&#34;&gt;Command-line deployment&lt;/h3&gt;
&lt;p&gt;For the first method, this follows a &lt;code&gt;git&lt;/code&gt; workflow for deployment. In
short you create a &lt;code&gt;git&lt;/code&gt; remote locally, login with the &lt;code&gt;heroku&lt;/code&gt; cli, and
push changes to the Heroku &lt;code&gt;git&lt;/code&gt; repo. An update to the remote triggers
a deployment and within a minute or two your app is publicly available!
There&#39;s more information available here in the
&lt;a href=&#34;https://devcenter.heroku.com/articles/git&#34;&gt;Heroku docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My steps were a bit different as I had already created a Heroku project
through the UI and already had a local &lt;code&gt;.git&lt;/code&gt; repo. I simply got the Heroku
&lt;code&gt;git&lt;/code&gt; URL through the Settings menu for the project; it should look like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://git.heroku.com/&amp;lt;project-name&amp;gt;.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now all you need to do is add it as a &lt;code&gt;git&lt;/code&gt; remote in your local &lt;code&gt;.git&lt;/code&gt; repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git remote add heroku https://git.heroku.com/&amp;lt;project-name&amp;gt;.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need to install the
&lt;a href=&#34;https://devcenter.heroku.com/articles/heroku-cli&#34;&gt;Heroku CLI&lt;/a&gt; (&lt;code&gt;heroku&lt;/code&gt;) and
login: &lt;code&gt;heroku login&lt;/code&gt;. After that is done, you deploy with a push:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git push heroku main
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;continuous-deployment&#34;&gt;Continuous deployment&lt;/h3&gt;
&lt;p&gt;The above step is great if you&#39;re learning to deploy your app for the first
time. However, this step is manual and it&#39;s tedious as you make improvements
to your app.&lt;/p&gt;
&lt;p&gt;Heroku has a couple of options for continuous deployment (CD). The first of
which is GitHub integration. Through the Heroku UI, the &amp;ldquo;Deploy&amp;rdquo; menu for
the project has configuration settings for deployment.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot3.png&#34; data-caption=&#34;GitHub integration in Heroku&#34;&gt;
&lt;img src=&#34;screenshot3.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GitHub integration in Heroku
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After you connect your Heroku app with your GitHub account, you can easily
enable CD with the &amp;ldquo;Enable Automatic Deploys&amp;rdquo; button.&lt;/p&gt;
&lt;p&gt;While this is great, I realized that this will re-deploy with any changes
to the GitHub repo. That is, even a simple change to the README.md or the
CHANGELOG would trigger a new deployment. Thus, a bit of control over
deployment is still needed.&lt;/p&gt;
&lt;p&gt;While developing this app, I followed a GitHub workflow where I created a new
branch which is then merged into the &lt;code&gt;main&lt;/code&gt; branch through a pull request
(PR). To distinguish changes to the code from other changes that does not
impact end users, I performed a &lt;code&gt;git tag&lt;/code&gt; which is related to a version
bump of the codebase. The advantage here is that I can easily trigger
deployment based on a new tag using GitHub Actions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Heroku Deployment

on:
  push:
    tags:
      - &#39;v*&#39;

jobs:
  heroku-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Heroku deployment
        uses: akhileshns/heroku-deploy@v3.12.12
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_app_name: &amp;quot;&amp;lt;project-name&amp;gt;&amp;quot;
          heroku_email: &amp;quot;myemail@dot.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need to create a Heroku API token (it&#39;s in the &amp;ldquo;Account Settings&amp;rdquo;
option of your Heroku account) and add it as a secret in the GitHub repo.
With the above workflow, I can easily trigger a deployment using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git push --tags
&lt;/code&gt;&lt;/pre&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot4.png&#34; data-caption=&#34;GitHub Actions with a Heroku deployment&#34;&gt;
&lt;img src=&#34;screenshot4.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GitHub Actions with a Heroku deployment
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;And that&#39;s how I built a &lt;code&gt;streamlit&lt;/code&gt; app that I easily deployed on Heroku
for free!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-important-for-end-users&#34;&gt;What&#39;s important for end users?&lt;/h2&gt;
&lt;p&gt;Recently I asked users for the next feature they would like to see.
With 40% of the vote, users felt that having a search tool to look at
individuals&amp;rsquo; salary data would be great.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;.&lt;a href=&#34;https://twitter.com/uarizona?ref_src=twsrc%5Etfw&#34;&gt;@uarizona&lt;/a&gt; salary app is all about the experience of the users, so help me prioritize what&amp;#39;s the next 😎 feature for 🚀!&lt;a href=&#34;https://t.co/sPs8xmkTbz&#34;&gt;https://t.co/sPs8xmkTbz&lt;/a&gt;&lt;a href=&#34;https://twitter.com/CAJUArizona?ref_src=twsrc%5Etfw&#34;&gt;@CAJUArizona&lt;/a&gt; &lt;a href=&#34;https://twitter.com/UCWArizona?ref_src=twsrc%5Etfw&#34;&gt;@UCWArizona&lt;/a&gt; &lt;a href=&#34;https://twitter.com/coba_uarizona?ref_src=twsrc%5Etfw&#34;&gt;@coba_uarizona&lt;/a&gt;&lt;/p&gt;&amp;mdash; Chun Ly, Ph.D. (@astrochunly) &lt;a href=&#34;https://twitter.com/astrochunly/status/1407485898485239809?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This data view is now available and even has the option to search
for all individuals in a department.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&#39;s next?&lt;/h2&gt;
&lt;h4 id=&#34;more-data-views&#34;&gt;More data views!&lt;/h4&gt;
&lt;p&gt;In 2020, UArizona completed the
&lt;a href=&#34;https://hr.arizona.edu/content/university-career-architecture-project-ucap&#34;&gt;University Career Architecture Project (UCAP)&lt;/a&gt;,
which map every employee to specific career streams. I recently submitted
a public request for FY2020-21 data that includes UCAP and other
demographics (e.g. sex, race, ethnicity). In fact,
&lt;a href=&#34;https://twitter.com/astrochunly/status/1392310191190732802&#34;&gt;UCAP was my original motivation for this app&lt;/a&gt;.
With a proper mapping of career streams, any UArizona member could
easily compare their salary against those across the campus with the
same job responsibilities.&lt;/p&gt;
&lt;p&gt;Once I have these data, I&#39;ll then add additional views to explore
the UCAP classification and other demographics.&lt;/p&gt;
&lt;h2 id=&#34;support-it&#34;&gt;Support it!&lt;/h2&gt;
&lt;p&gt;If you found this &lt;strong&gt;open-source&lt;/strong&gt; software or this tutorial post useful,
consider sponsoring me through GitHub with a small donation. Your name will
be included as a sponsor on the app&#39;s main page!&lt;/p&gt;
&lt;iframe src=&#34;https://github.com/sponsors/astrochun/button&#34;
title=&#34;Sponsor astrochun&#34; height=&#34;35&#34; width=&#34;116&#34; style=&#34;border: 0;&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>Preserving your voting records on Vox Charta with Python</title>
      <link>https://astrochun.github.io/post/voxcharta_preserve/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/voxcharta_preserve/</guid>
      <description>&lt;p&gt;It&#39;s been a few months since I have done a tech/hacking post. I figured it might
be worthwhile to do a quick one on the recent web scraping work that I did.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://voxcharta.org&#34;&gt;Vox Charta&lt;/a&gt;, a web application that allowed astronomers
to vote and discuss about the latest manuscripts made available on
&lt;a href=&#34;https://arxiv.org&#34;&gt;arXiv&lt;/a&gt;, was conceived in 2009 by
&lt;a href=&#34;https://github.com/guillochon&#34;&gt;James Guillochon&lt;/a&gt;.
It was used by thousands of astronomers internationally as there was no
other options available for several years. Unfortunately, it was
&lt;a href=&#34;https://twitter.com/astrocrash/status/1287053013589385217&#34;&gt;announced&lt;/a&gt;
that the service would be sunsetted on December 31, 2020.&lt;/p&gt;
&lt;p&gt;Personally I used Vox Charta since 2011 and have utilized its voting method to
record astronomy papers of interest to me. When I heard the news of the sunset
I immediately knew that I needed to #preserve my voting records. There were
over 2,000 papers of interest to me!&lt;/p&gt;
&lt;p&gt;So I went forth and built a simple Python software to extract that information.
The &amp;ldquo;My Voting Records&amp;rdquo; content was stored in HTML, so it made sense to use
&lt;a href=&#34;https://www.crummy.com/software/BeautifulSoup/bs4/doc/&#34;&gt;&lt;code&gt;BeautifulSoup&lt;/code&gt;&lt;/a&gt;
for extraction! It was quite fun to solve this problem; this was my first
time building a web scraping tool. Loading the content was pretty straightforward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re
from bs4 import BeautifulSoup
import json
import pandas as pd

class Extract:
    def __init__(self, filename, json_outfile, csv_outfile):
    
            self.log = log
            self.filename = filename
            self.json_outfile = json_outfile
            self.csv_outfile = csv_outfile
    
            self.content = self.import_data()

    def import_data(self):
        &amp;quot;&amp;quot;&amp;quot;Import data&amp;quot;&amp;quot;&amp;quot;

        with open(self.filename, &#39;r&#39;) as f:
            content = f.read()
        f.close()

        return content

    def soup_it(self):
        &amp;quot;&amp;quot;&amp;quot;Construct BeautifulSoup data structure&amp;quot;&amp;quot;&amp;quot;

        page_content = BeautifulSoup(self.content, &amp;quot;html.parser&amp;quot;)

        return page_content
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the exported HTML was not as well structured, some trial and error
was needed to figure out how to extract specific content. In the end, I was
able to scrape the arXiv ID, title, author list, author affiliation, abstract,
arXiv categories, links, comments, and many other information. The method
to retrieve the voting records is too long to put it here, but you can see
it &lt;a href=&#34;https://github.com/astrochun/voxcharta-my-voting-record/blob/main/voxcharta_my_voting_record/extract.py#L77-L163&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then, I added the ability to export the content to two machine-readable forms.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def export_data(self, records_dict):
        &amp;quot;&amp;quot;&amp;quot;Write JSON and csv files&amp;quot;&amp;quot;&amp;quot;

        with open(self.json_outfile, &#39;w&#39;) as outfile:
            json.dump(records_dict, outfile, indent=4)

        df = pd.DataFrame.from_dict(records_dict, orient=&#39;index&#39;)
        df.to_csv(self.csv_outfile, index=False)

        # Write arxiv_id list
        arxiv_outfile = self.csv_outfile.replace(&#39;.csv&#39;, &#39;_arxiv.txt&#39;)
        clean_df = df.loc[(df[&#39;arxiv_id&#39;] != &#39;&#39;) &amp;amp; (df[&#39;arxiv_id&#39;].notna())]
        clean_df.to_csv(arxiv_outfile, index=False, header=False,
                        columns=[&#39;arxiv_id&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, two types of files are provided: JSON and
CSV. The latter is done with &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;/a&gt;.
An example of the exported JSON output is available below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;voxcharta_json.jpg&#34; &gt;
&lt;img src=&#34;voxcharta_json.jpg&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;I then provided my voting records to the maintainers of the
&lt;a href=&#34;https://www.benty-fields.com/&#34;&gt;Benty-Fields&lt;/a&gt; web application. It
has many features that are similar to Vox Charta by allowing for
discussions and voting. In the end, those records were transferred
over!&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;benty_fields.png&#34; &gt;
&lt;img src=&#34;benty_fields.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;If you&#39;re interested in using this code, it is available
&lt;a href=&#34;https://github.com/astrochun/voxcharta-my-voting-record&#34;&gt;here&lt;/a&gt;.
The code is made available under a
&lt;a href=&#34;https://github.com/astrochun/voxcharta-my-voting-record/blob/main/LICENSE&#34;&gt;GNU GPL-3.0&lt;/a&gt;
License so other astronomers can preserve their precious Vox Charta records.&lt;/p&gt;
&lt;p&gt;The README.md provides more details, but here are some quick
installation and execution steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download the complete webpage view from the &amp;ldquo;My Voting Record&amp;rdquo; page before
the site is permanently down&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clone the repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/astrochun/voxcharta-my-voting-record
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build it (preferably in a contained environment)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd voxcharta-my-voting-record
$ (sudo) python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can then easily execute the main script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ vox_run -f myvotingrecords.htm
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Many thanks to James Guillochon for being the selfless maintainer of Vox
Charta for over a decade. I&#39;m also thankful to the Benty-Fields maintainers
for their help.&lt;/p&gt;
&lt;p&gt;If you found this &lt;strong&gt;open-source&lt;/strong&gt; software useful, consider sponsoring me
through GitHub.&lt;/p&gt;
&lt;iframe src=&#34;https://github.com/sponsors/astrochun/button&#34;
title=&#34;Sponsor astrochun&#34; height=&#34;35&#34; width=&#34;116&#34; style=&#34;border: 0;&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>When the pandemic hit, I used my data analysis and Google Sheet skills to inform</title>
      <link>https://astrochun.github.io/post/covid19_research/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/covid19_research/</guid>
      <description>&lt;p&gt;Without a doubt, COVID-19 has impacted every aspect of life, especially higher
education. Earlier this year, the University of Arizona (UofA)
&lt;a href=&#34;https://view.comms.arizona.edu/?qs=af589cd97cb2617e9fdf2ab657a0d6345f451dbc795ba254bc81195f60626149c14bc1d2f83064236848f4247e26ebfa57a0c9858c2d3201e12beef6136f1cf0df93fc33ec93b71912398904fbc91c6b&#34;&gt;announced&lt;/a&gt;
it would start the Fall 2020 semester in a hybrid mode of in-person and online
instruction, and had prepared for this re-entry in a number of ways.
One of those priorities is testing of students, staff, and faculty to identify
active cases. The UofA was not alone, as many other universities had implemented
their own COVID-19 testing program.&lt;/p&gt;
&lt;p&gt;In late August, the UofA provided summary data containing new daily tests
(for the previous day) and cumulative numbers (July 31&amp;ndash;) on their main
&lt;a href=&#34;https://covid19.arizona.edu/updates&#34;&gt;COVID-19 Updates webpage&lt;/a&gt;.  Over time, it
was clear that the numbers were not improving, but I grew frustrated as it was
difficult to see &lt;em&gt;quantitative&lt;/em&gt; trends.  Prior to the start of the semester, it
was said during a COVID-19 webinar that the
&lt;a href=&#34;https://covid19.arizona.edu/dashboard&#34;&gt;COVID-19 dashboard&lt;/a&gt; would be made
available in late August.  So on September 3, I decided to start an independent
volunteer project where I started to aggregate the summary data.  I announced
it on &lt;a href=&#34;https://twitter.com/astrochunly/status/1301532415773478912&#34;&gt;Twitter&lt;/a&gt;, and
started with whatever screenshots I had saved and began putting the information
in a &lt;a href=&#34;https://bit.ly/UArizona_COVID_Testing&#34;&gt;Google Sheet&lt;/a&gt;, which I made &lt;em&gt;publicly&lt;/em&gt;
available for &lt;strong&gt;everyone&lt;/strong&gt; to see. Over time, I had the help of a few UofA colleagues
that connected with me through social media:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Jill McCleary, Deputy Director and Acting Head at &lt;a href=&#34;https://artmuseum.arizona.edu/&#34;&gt;UofA Museum of Art&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/ironyofprint&#34;&gt;Cheryl Knott&lt;/a&gt;, Professor in the School of Information&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/scastiello&#34;&gt;Santiago Castiello-Gutiérrez&lt;/a&gt;, Ph.D. candidate in the Higher Education program&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I am so thankful to them as this project would not have been possible without them.&lt;/p&gt;
&lt;p&gt;I chose Google Sheet for many reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Easy to use&lt;/li&gt;
&lt;li&gt;Easy to publish&lt;/li&gt;
&lt;li&gt;Easy to collaborate&lt;/li&gt;
&lt;li&gt;Easy to make charts/graphs&lt;/li&gt;
&lt;li&gt;As a programmer, built-in version control&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This project was very much &lt;a href=&#34;https://en.wikipedia.org/wiki/Open_science&#34;&gt;#openscience&lt;/a&gt;
as hundreds of users checked out the spreadsheet daily to look at the latest updates.
Since many users viewed it anonymously, it was hard to get exact numbers, but this
was what I found from Twitter, Google, and Bitly tracking.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I estimate a total of 545 clicks on either the original Google or Bitly link&lt;/li&gt;
&lt;li&gt;393 (72%) of those clicks were through Twitter and Facebook&lt;/li&gt;
&lt;li&gt;The spreadsheet was shared via other platforms (SMS, email, etc.) for nearly
152 additional clicks (28%)&lt;/li&gt;
&lt;li&gt;There were 146 unique non-anonymous viewers to the Google Sheet&lt;/li&gt;
&lt;li&gt;Using Bitly, I was able to see that viewers included those in at least ten
different foreign countries.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This all happened mostly during a nearly two-week period before the COVID-19 dashboard
went live on September 16.&lt;/p&gt;
&lt;p&gt;The source of information came from my colleagues,
&lt;a href=&#34;https://www.youtube.com/c/universityofarizona&#34;&gt;UofA&#39;s weekly re-entry broadcasting&lt;/a&gt;,
and using the &lt;a href=&#34;https://archive.org/web/&#34;&gt;WayBackMachine&lt;/a&gt; to go back and look at the
websites. After all the data were aggregated, there was a lot of quality control as some
numbers disagreed, and that needed to be recorded in the Google Sheet with
in-cell comments and source identification.  This is what we call good
&lt;a href=&#34;https://en.wikipedia.org/wiki/Data_management&#34;&gt;data management&lt;/a&gt; practice.
A preview of the machine-readable consolidated spreadsheet is available below:&lt;/p&gt;
&lt;p&gt;&lt;iframe src=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQ3asdKKUNbQj9tpMo8dircaSVxDHJR0fKo8L9cLQ3TMqHb-KBzHmCT_iRl2ie6xbtHEV3V5EHZjuo-/pubhtml?gid=880188181&amp;single=true&#34; frameborder=&#34;0&#34; width=&#34;715&#34; height=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;Then, after I felt confident of all the numbers, &lt;a href=&#34;featured.png&#34;&gt;I put together graphs that summarized the testing data.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The final culmination of this hard work by my colleagues and I is publishing
the final table and visualization in the
&lt;a href=&#34;https://arizona.figshare.com&#34;&gt;UofA Research Data Repository&lt;/a&gt;. The work is titled
&lt;a href=&#34;https://doi.org/10.25422/azu.data.12966581&#34;&gt;&amp;ldquo;Independent Data Aggregation, Quality Control and Visualization of University of Arizona COVID-19 Re-Entry Testing Data&amp;rdquo;&lt;/a&gt; and is made available
under a &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/deed.en&#34;&gt;Creative Commons Zero (CC0)&lt;/a&gt;
public domain license:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;iframe src=&#34;https://widgets.figshare.com/articles/12966581/embed?show_title=true&#34; width=&#34;568&#34; height=&#34;351&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;I really enjoyed being able to share what I learned with the UofA community.
I found myself needing to make this happen so everyone knew how fast COVID-19
was spreading in the UofA community. We &lt;strong&gt;all&lt;/strong&gt; need to pull together to
fight back, and gathering, checking and illustrating information was the best
way for me to go about it.&lt;/p&gt;
&lt;p&gt;#StaySafe, #BearDown, and #MaskUp!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Syncing an Overleaf project with your GitHub account</title>
      <link>https://astrochun.github.io/post/overleaf_github_git_sync/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/overleaf_github_git_sync/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.overleaf.com&#34;&gt;Overleaf&lt;/a&gt; is an environment for researchers to
collaboratively write their manuscripts and papers with $\LaTeX$. It is
an amazing tool, but I have found that it has its limitations. For example,
if you edit within their web UI, the changes may show up as separate commits
in their History page.  One of the benefits of Overleaf is that it is
integrated with &lt;code&gt;git&lt;/code&gt;.  This is great as you can maintain version control
and document changes to your collaborators.&lt;/p&gt;
&lt;p&gt;With the power of &lt;code&gt;git&lt;/code&gt;, I wanted to find a way to bring all the version
control to &lt;a href=&#34;https://www.github.com&#34;&gt;GitHub&lt;/a&gt; and their nice UI to illustrate
changes.&lt;/p&gt;
&lt;p&gt;I came across this &lt;a href=&#34;https://ineed.coffee/3454/how-to-synchronize-an-overleaf-latex-paper-with-a-github-repository/&#34;&gt;tutorial&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;The trick is using the default &lt;code&gt;git remote&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Sign up for a free account at &lt;a href=&#34;https://www.overleaf.com&#34;&gt;Overleaf&lt;/a&gt;
and create a new project (I recommend the &amp;ldquo;Example Project&amp;rdquo; option):&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;new_project.png&#34; data-caption=&#34;Step 1: Create a new Overleaf project&#34;&gt;
&lt;img src=&#34;new_project.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 1: Create a new Overleaf project
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Retrieve the &lt;code&gt;git clone&lt;/code&gt; command by selecting the Menu button in the
upper left and the &lt;code&gt;Git&lt;/code&gt; option:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;git_clone.png&#34; data-caption=&#34;Step 2: Retrieve git clone command&#34;&gt;
&lt;img src=&#34;git_clone.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 2: Retrieve &lt;code&gt;git clone&lt;/code&gt; command
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;You should see a pop-up window containing something similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;               git clone https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I use &lt;code&gt;&amp;lt;ABCDEF123456&amp;gt;&lt;/code&gt; as a placeholder for the real path throughout this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Open up a command-line terminal and execute this &lt;code&gt;git clone&lt;/code&gt; command and
finish it with a more meaningful project name:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; git_integration_project
$ cd git_integration_project
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You now have a copy of your Overleaf project locally! You can open the content
with your favorite text editor, and &lt;code&gt;git add&lt;/code&gt;, &lt;code&gt;git commit&lt;/code&gt;, and &lt;code&gt;git push&lt;/code&gt;
new changes to Overleaf.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: To take this one step further and mirror a copy on GitHub,
let&#39;s create an &lt;strong&gt;empty&lt;/strong&gt; &lt;a href=&#34;https://github.com/new&#34;&gt;new GitHub repository&lt;/a&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;github_new.png&#34; data-caption=&#34;Step 4: Create a new empty GitHub repository&#34;&gt;
&lt;img src=&#34;github_new.png&#34; alt=&#34;&#34; width=&#34;75%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 4: Create a new empty GitHub repository
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Note that this is entirely empty. This is very important as the content
will be populated from your Overleaf repository! Also, I made it a &lt;em&gt;private&lt;/em&gt;
repository. You might find that important as well since you might be
working on a paper that you want to keep private.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Within the &lt;code&gt;git_integration project&lt;/code&gt; folder, create a new &lt;code&gt;git remote&lt;/code&gt;
called &lt;code&gt;github&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote add github https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&#39;s look at our &lt;code&gt;git remote -v&lt;/code&gt; setup:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote -v
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (fetch)
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (push)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (fetch)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (push)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Let us now replicate what we have on Overleaf to GitHub:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git push github (master)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you refresh your GitHub repository, you should see all the $\LaTeX$ files.&lt;/p&gt;
&lt;p&gt;Of course, it would be nice to be able to push to both git repositories instead
of having to do:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git push origin master
$ git push github master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: To do this, let&#39;s create another &lt;code&gt;git remote&lt;/code&gt; called &lt;code&gt;both&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote add both https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt;
$ git remote set-url --add --push both https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt;
$ git remote set-url --add --push both https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us inspect the &lt;code&gt;git remote -v&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote -v
both	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (fetch)
both	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (push)
both	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (push)
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (fetch)
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (push)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (fetch)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (push)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&#39;s it! Now, any &lt;code&gt;git push both (master)&lt;/code&gt; will update both Overleaf and
GitHub.&lt;/p&gt;
&lt;p&gt;Now $\LaTeX$ + &lt;code&gt;git push&lt;/code&gt; on!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
