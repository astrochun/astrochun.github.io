<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Chun Ly</title>
    <link>https://astrochun.github.io/</link>
      <atom:link href="https://astrochun.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Chun Ly</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>MIT License</copyright><lastBuildDate>Sun, 29 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://astrochun.github.io/img/icon-192.png</url>
      <title>Chun Ly</title>
      <link>https://astrochun.github.io/</link>
    </image>
    
    <item>
      <title>Building a streamlit app to serve UArizona salary data and deploying it on Heroku</title>
      <link>https://astrochun.github.io/post/salary_app/</link>
      <pubDate>Sun, 29 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/salary_app/</guid>
      <description>&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.wildcat.arizona.edu/&#34;&gt;Daily Wildcat&lt;/a&gt; of the
&lt;a href=&#34;https://arizona.edu&#34;&gt;University of Arizona&lt;/a&gt; (UArizona) has routinely provided
the salary of UArizona employees for nearly a decade through public requests.
While this allowed for the easy viewing of information for an individual,
the analysis from the Daily Wildcat was rather limited. As such, I wanted to
build a data visualization app that would allow anyone to explore the data for
100% transparency. I call it &lt;code&gt;sapp4ua&lt;/code&gt;, which is short for the
salary app for UArizona. It&#39;s available at &lt;a href=&#34;https://sapp4ua.herokuapp.com&#34;&gt;https://sapp4ua.herokuapp.com&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;The salary data were available as machine-readable CSV files, so it made sense
to build a Python application that uses &lt;code&gt;pandas&lt;/code&gt; to extract, transform, and
load (ETL) the data. I heard about &lt;code&gt;streamlit&lt;/code&gt;, a Python web framework that
would make it faster to build such an app. Essentially &lt;code&gt;streamlit&lt;/code&gt; provides a
RESTful application programming interface (API) with a front-end layer that
interacts with Python scripts that do data analysis and visualization. It
took me about a weekend to understand &lt;code&gt;streamlit&lt;/code&gt; and its &amp;ldquo;widgets&amp;rdquo;, which
are interactive tools that could be used to perform ETL actions with the data.
From there, I put together the software to load the data and provide different
&amp;ldquo;data views&amp;rdquo; that were different interactions with the data. The code is
available on &lt;a href=&#34;https://www.github.com/astrochun/uarizona-salary-app&#34;&gt;GitHub&lt;/a&gt;
under an MIT License. There are many resources and blogs about building a
&lt;code&gt;streamlit&lt;/code&gt; app, so I won&#39;t go in details here, but I will illustrate some
aspects:&lt;/p&gt;
&lt;p&gt;Loading in data can be simple.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
import pandas as pd
import streamlit as st

# List of fiscal years
FY_LIST = [
    &#39;FY2019-20&#39;, &#39;FY2018-19&#39;, &#39;FY2017-18&#39;, &#39;FY2016-17&#39;,
    &#39;FY2014-15&#39;, &#39;FY2013-14&#39;, &#39;FY2011-12&#39;,
]


@st.cache
def load_data():

    file_id = {
        &#39;FY2019-20&#39;: &#39;1d2l29_T-mOh05bglPlwAFlzeV1PIkRXd&#39;,
        &#39;FY2018-19&#39;: &#39;1paxrUyW1wZuK3bjSL_L7ckKEC6xslZJe&#39;,
        &#39;FY2017-18&#39;: &#39;1AnRaPpbRTLVyqdeqe6vkPMYgbNnw9zia&#39;,
        &#39;FY2016-17&#39;: &#39;1rXBuuXit5oWKtfnA05gsNtsWAyESeIs2&#39;,
        &#39;FY2014-15&#39;: &#39;1ZANVDr6Kw40MJYiOENWbLMTFEMWyf7f4&#39;,
        &#39;FY2013-14&#39;: &#39;1rQ8A2CIVhDYu0lESKVh72h6VUd8gIEFl&#39;,
        &#39;FY2011-12&#39;: &#39;1fQOzEHiOvc_H1NcLMlK3KVV1DJkRbRuX&#39;,
    }

    data_dict = {}
    for year in FY_LIST:
        data_dict[year] = pd.read_csv(
            f&#39;https://drive.google.com/uc?id={file_id[year]}&#39;
        )

    return data_dict


def main():

    # Load data
    data_dict = load_data()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above code loads each previous year of available data into a &lt;code&gt;dict&lt;/code&gt;
of &lt;code&gt;pandas&lt;/code&gt; DataFrames. The &lt;code&gt;st.cache&lt;/code&gt; decorator ensures that the data is
loaded into memory only once. This ensures a shorter response time when it&#39;s
deployed and users are accessing the app frequently. Here, I made the data
publicly available in Google Drive. I could have gone with an SQL server,
but given the  small record sizes (~10,000 records), and the reliability
of Google Drive, this worked well for an MVP.&lt;/p&gt;
&lt;p&gt;To select from the data, I utilized &lt;code&gt;streamlit&lt;/code&gt; sidebar widgets to add the
layer for selection. For example, the following selects the data for one
year:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def select_fiscal_year() -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Sidebar widget to select fiscal year&amp;quot;&amp;quot;&amp;quot;

    st.sidebar.markdown(&#39;### Select fiscal year:&#39;)
    fy_select = st.sidebar.selectbox(&#39;&#39;, FY_LIST, index=0).split(&#39; &#39;)[0]

    return fy_select


def main():
    # Load data
    data_dict = load_data()

    fy_select = select_fiscal_year()

    # Select dataframe
    df = data_dict[fy_select]
    st.sidebar.text(f&amp;quot;{fy_select} data imported!&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This results in something that looks like this:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot1.png&#34; data-caption=&#34;Sidebar tool selection for fiscal year&#34;&gt;
&lt;img src=&#34;screenshot1.png&#34; alt=&#34;&#34; width=&#34;40%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sidebar tool selection for fiscal year
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;As discussed earlier, interactions with the data is done with different views.
Currently, I have six of them:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Trends: General facts and numbers (e.g. number of employees, salary budget),
for each fiscal year.&lt;/li&gt;
&lt;li&gt;Salary Summary: Statistics and percentile salary data, includes salary
histogram.&lt;/li&gt;
&lt;li&gt;Highest Earners: Extract data above a minimum salary.&lt;/li&gt;
&lt;li&gt;College/Division Data: Similar to Salary Summary but extracted for each
college(s)/division(s).&lt;/li&gt;
&lt;li&gt;Department Data: Similar to Salary Summary but extracted for each
department(s).&lt;/li&gt;
&lt;li&gt;Individual Search: Search for all data for individuals or by looking at
each department. You can read more &lt;a href=&#34;#whats-important-for-end-users&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This can easily be done on the sidebar and then the main page is loaded:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;DATA_VIEWS = [
    &#39;About&#39;, &#39;Individual Search (NEW)&#39;, &#39;Trends&#39;, &#39;Salary Summary&#39;,
    &#39;Highest Earners&#39;, &#39;College/Division Data&#39;, &#39;Department Data&#39;,
]

def select_data_view() -&amp;gt; str:
    &amp;quot;&amp;quot;&amp;quot;Sidebar widget to select your data view&amp;quot;&amp;quot;&amp;quot;

    st.sidebar.markdown(&#39;### Select your data view:&#39;)
    view_select = st.sidebar.selectbox(&#39;&#39;, DATA_VIEWS, index=0)

    return view_select


def main():
    # Sidebar, select data view
    view_select = select_data_view()

    if view_select == &#39;About&#39;:
        views.about_page()

    if view_select == &#39;Trends&#39;:
        views.trends_page(data_dict, pay_norm)

    if view_select == &#39;Salary Summary&#39;:
        views.salary_summary_page(df, pay_norm)

    if view_select == &#39;Highest Earners&#39;:
        views.highest_earners_page(df)

    # Select by College Name
    if view_select == &#39;College/Division Data&#39;:
        views.subset_select_data_page(df, COLLEGE_NAME, &#39;college&#39;,
                                      pay_norm)

    # Select by Department Name
    if view_select == &#39;Department Data&#39;:
        views.subset_select_data_page(df, &#39;Department&#39;, &#39;department&#39;,
                                      pay_norm)

    # Load individual search
    if view_select == &#39;Individual Search&#39;:
        views.individual_search_page(data_dict, unique_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here &lt;code&gt;views&lt;/code&gt; is a Python module and each function displays
different content in the main page. Here&#39;s a screenshot for the
Salary Summary page:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot2.png&#34; data-caption=&#34;One of the data views from sapp4ua.&#34;&gt;
&lt;img src=&#34;screenshot2.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    One of the data views from &lt;code&gt;sapp4ua&lt;/code&gt;.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;deploying-it-on-heroku&#34;&gt;Deploying it on Heroku&lt;/h2&gt;
&lt;p&gt;With a functional app, it&#39;s easy to have it locally deployed with the
following command:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;streamlit run salary_app:main
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To make it available on a production instance, I looked into hosting services
(called Platform as a Service), and it turned out that
&lt;a href=&#34;https://heroku.com/about&#34;&gt;Heroku&lt;/a&gt; has a free tier for serverless deployment.
It seems as though it&#39;s one of the popular options among &lt;code&gt;streamlit&lt;/code&gt; users to
deploy their apps. Note that &lt;code&gt;streamlit&lt;/code&gt; runs a web server so static
site hosting services (e.g., GitHub Pages) will not work. To get started,
first &lt;a href=&#34;https://signup.heroku.com/&#34;&gt;sign-up&lt;/a&gt; for a Heroku account. To create
your first Heroku project, select the &amp;ldquo;Create New App&amp;rdquo; option from the New
button. Here the project name needs to be unique across all of Heroku
because it will point to &lt;code&gt;https://&amp;lt;project-name&amp;gt;.herokuapp.com&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now you have a project that can be triggered to deploy after any codebase
changes. What I learned is that Heroku is developer friendly.
It has at least three ways to deploy it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Manually using &lt;code&gt;git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Using GitHub integration&lt;/li&gt;
&lt;li&gt;Using a GitHub Action&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;commandline-deployment&#34;&gt;Command-line deployment&lt;/h3&gt;
&lt;p&gt;For the first method, this follows a &lt;code&gt;git&lt;/code&gt; workflow for deployment. In
short you create a &lt;code&gt;git&lt;/code&gt; remote locally, login with the &lt;code&gt;heroku&lt;/code&gt; cli, and
push changes to the Heroku &lt;code&gt;git&lt;/code&gt; repo. An update to the remote triggers
a deployment and within a minute or two your app is publicly available!
There&#39;s more information available here in the
&lt;a href=&#34;https://devcenter.heroku.com/articles/git&#34;&gt;Heroku docs&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;My steps were a bit different as I had already created a Heroku project
through the UI and already had a local &lt;code&gt;.git&lt;/code&gt; repo. I simply got the Heroku
&lt;code&gt;git&lt;/code&gt; URL through the Settings menu for the project; it should look like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://git.heroku.com/&amp;lt;project-name&amp;gt;.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now all you need to do is add it as a &lt;code&gt;git&lt;/code&gt; remote in your local &lt;code&gt;.git&lt;/code&gt; repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git remote add heroku https://git.heroku.com/&amp;lt;project-name&amp;gt;.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need to install the
&lt;a href=&#34;https://devcenter.heroku.com/articles/heroku-cli&#34;&gt;Heroku CLI&lt;/a&gt; (&lt;code&gt;heroku&lt;/code&gt;) and
login: &lt;code&gt;heroku login&lt;/code&gt;. After that is done, you deploy with a push:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git push heroku main
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;continuous-deployment&#34;&gt;Continuous deployment&lt;/h3&gt;
&lt;p&gt;The above step is great if you&#39;re learning to deploy your app for the first
time. However, this step is manual and it&#39;s tedious as you make improvements
to your app.&lt;/p&gt;
&lt;p&gt;Heroku has a couple of options for continuous deployment (CD). The first of
which is GitHub integration. Through the Heroku UI, the &amp;ldquo;Deploy&amp;rdquo; menu for
the project has configuration settings for deployment.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot3.png&#34; data-caption=&#34;GitHub integration in Heroku&#34;&gt;
&lt;img src=&#34;screenshot3.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GitHub integration in Heroku
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;After you connect your Heroku app with your GitHub account, you can easily
enable CD with the &amp;ldquo;Enable Automatic Deploys&amp;rdquo; button.&lt;/p&gt;
&lt;p&gt;While this is great, I realized that this will re-deploy with any changes
to the GitHub repo. That is, even a simple change to the README.md or the
CHANGELOG would trigger a new deployment. Thus, a bit of control over
deployment is still needed.&lt;/p&gt;
&lt;p&gt;While developing this app, I followed a GitHub workflow where I created a new
branch which is then merged into the &lt;code&gt;main&lt;/code&gt; branch through a pull request
(PR). To distinguish changes to the code from other changes that does not
impact end users, I performed a &lt;code&gt;git tag&lt;/code&gt; which is related to a version
bump of the codebase. The advantage here is that I can easily trigger
deployment based on a new tag using GitHub Actions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Heroku Deployment

on:
  push:
    tags:
      - &#39;v*&#39;

jobs:
  heroku-deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2
      - name: Heroku deployment
        uses: akhileshns/heroku-deploy@v3.12.12
        with:
          heroku_api_key: ${{ secrets.HEROKU_API_KEY }}
          heroku_app_name: &amp;quot;&amp;lt;project-name&amp;gt;&amp;quot;
          heroku_email: &amp;quot;myemail@dot.com&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You will need to create a Heroku API token (it&#39;s in the &amp;ldquo;Account Settings&amp;rdquo;
option of your Heroku account) and add it as a secret in the GitHub repo.
With the above workflow, I can easily trigger a deployment using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;git push --tags
&lt;/code&gt;&lt;/pre&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;screenshot4.png&#34; data-caption=&#34;GitHub Actions with a Heroku deployment&#34;&gt;
&lt;img src=&#34;screenshot4.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GitHub Actions with a Heroku deployment
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;And that&#39;s how I built a &lt;code&gt;streamlit&lt;/code&gt; app that I easily deployed on Heroku
for free!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;whats-important-for-end-users&#34;&gt;What&#39;s important for end users?&lt;/h2&gt;
&lt;p&gt;Recently I asked users for the next feature they would like to see.
With 40% of the vote, users felt that having a search tool to look at
individuals&amp;rsquo; salary data would be great.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;.&lt;a href=&#34;https://twitter.com/uarizona?ref_src=twsrc%5Etfw&#34;&gt;@uarizona&lt;/a&gt; salary app is all about the experience of the users, so help me prioritize what&amp;#39;s the next 😎 feature for 🚀!&lt;a href=&#34;https://t.co/sPs8xmkTbz&#34;&gt;https://t.co/sPs8xmkTbz&lt;/a&gt;&lt;a href=&#34;https://twitter.com/CAJUArizona?ref_src=twsrc%5Etfw&#34;&gt;@CAJUArizona&lt;/a&gt; &lt;a href=&#34;https://twitter.com/UCWArizona?ref_src=twsrc%5Etfw&#34;&gt;@UCWArizona&lt;/a&gt; &lt;a href=&#34;https://twitter.com/coba_uarizona?ref_src=twsrc%5Etfw&#34;&gt;@coba_uarizona&lt;/a&gt;&lt;/p&gt;&amp;mdash; Chun Ly, Ph.D. (@astrochunly) &lt;a href=&#34;https://twitter.com/astrochunly/status/1407485898485239809?ref_src=twsrc%5Etfw&#34;&gt;June 22, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This data view is now available and even has the option to search
for all individuals in a department.&lt;/p&gt;
&lt;h2 id=&#34;whats-next&#34;&gt;What&#39;s next?&lt;/h2&gt;
&lt;h4 id=&#34;more-data-views&#34;&gt;More data views!&lt;/h4&gt;
&lt;p&gt;In 2020, UArizona completed the
&lt;a href=&#34;https://hr.arizona.edu/content/university-career-architecture-project-ucap&#34;&gt;University Career Architecture Project (UCAP)&lt;/a&gt;,
which map every employee to specific career streams. I recently submitted
a public request for FY2020-21 data that includes UCAP and other
demographics (e.g. sex, race, ethnicity). In fact,
&lt;a href=&#34;https://twitter.com/astrochunly/status/1392310191190732802&#34;&gt;UCAP was my original motivation for this app&lt;/a&gt;.
With a proper mapping of career streams, any UArizona member could
easily compare their salary against those across the campus with the
same job responsibilities.&lt;/p&gt;
&lt;p&gt;Once I have these data, I&#39;ll then add additional views to explore
the UCAP classification and other demographics.&lt;/p&gt;
&lt;h2 id=&#34;support-it&#34;&gt;Support it!&lt;/h2&gt;
&lt;p&gt;If you found this &lt;strong&gt;open-source&lt;/strong&gt; software or this tutorial post useful,
consider sponsoring me through GitHub with a small donation. Your name will
be included as a sponsor on the app&#39;s main page!&lt;/p&gt;
&lt;iframe src=&#34;https://github.com/sponsors/astrochun/button&#34;
title=&#34;Sponsor astrochun&#34; height=&#34;35&#34; width=&#34;116&#34; style=&#34;border: 0;&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>The Metal Abundances across Cosmic Time (MACT) Survey. III - The relationship between stellar mass and star formation rate in extremely low-mass galaxies</title>
      <link>https://astrochun.github.io/publication/2021-mnras-501-2231-s/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2021-mnras-501-2231-s/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Preserving your voting records on Vox Charta with Python</title>
      <link>https://astrochun.github.io/post/voxcharta_preserve/</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/voxcharta_preserve/</guid>
      <description>&lt;p&gt;It&#39;s been a few months since I have done a tech/hacking post. I figured it might
be worthwhile to do a quick one on the recent web scraping work that I did.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://voxcharta.org&#34;&gt;Vox Charta&lt;/a&gt;, a web application that allowed astronomers
to vote and discuss about the latest manuscripts made available on
&lt;a href=&#34;https://arxiv.org&#34;&gt;arXiv&lt;/a&gt;, was conceived in 2009 by
&lt;a href=&#34;https://github.com/guillochon&#34;&gt;James Guillochon&lt;/a&gt;.
It was used by thousands of astronomers internationally as there was no
other options available for several years. Unfortunately, it was
&lt;a href=&#34;https://twitter.com/astrocrash/status/1287053013589385217&#34;&gt;announced&lt;/a&gt;
that the service would be sunsetted on December 31, 2020.&lt;/p&gt;
&lt;p&gt;Personally I used Vox Charta since 2011 and have utilized its voting method to
record astronomy papers of interest to me. When I heard the news of the sunset
I immediately knew that I needed to #preserve my voting records. There were
over 2,000 papers of interest to me!&lt;/p&gt;
&lt;p&gt;So I went forth and built a simple Python software to extract that information.
The &amp;ldquo;My Voting Records&amp;rdquo; content was stored in HTML, so it made sense to use
&lt;a href=&#34;https://www.crummy.com/software/BeautifulSoup/bs4/doc/&#34;&gt;&lt;code&gt;BeautifulSoup&lt;/code&gt;&lt;/a&gt;
for extraction! It was quite fun to solve this problem; this was my first
time building a web scraping tool. Loading the content was pretty straightforward:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import re
from bs4 import BeautifulSoup
import json
import pandas as pd

class Extract:
    def __init__(self, filename, json_outfile, csv_outfile):
    
            self.log = log
            self.filename = filename
            self.json_outfile = json_outfile
            self.csv_outfile = csv_outfile
    
            self.content = self.import_data()

    def import_data(self):
        &amp;quot;&amp;quot;&amp;quot;Import data&amp;quot;&amp;quot;&amp;quot;

        with open(self.filename, &#39;r&#39;) as f:
            content = f.read()
        f.close()

        return content

    def soup_it(self):
        &amp;quot;&amp;quot;&amp;quot;Construct BeautifulSoup data structure&amp;quot;&amp;quot;&amp;quot;

        page_content = BeautifulSoup(self.content, &amp;quot;html.parser&amp;quot;)

        return page_content
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Because the exported HTML was not as well structured, some trial and error
was needed to figure out how to extract specific content. In the end, I was
able to scrape the arXiv ID, title, author list, author affiliation, abstract,
arXiv categories, links, comments, and many other information. The method
to retrieve the voting records is too long to put it here, but you can see
it &lt;a href=&#34;https://github.com/astrochun/voxcharta-my-voting-record/blob/main/voxcharta_my_voting_record/extract.py#L77-L163&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Then, I added the ability to export the content to two machine-readable forms.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def export_data(self, records_dict):
        &amp;quot;&amp;quot;&amp;quot;Write JSON and csv files&amp;quot;&amp;quot;&amp;quot;

        with open(self.json_outfile, &#39;w&#39;) as outfile:
            json.dump(records_dict, outfile, indent=4)

        df = pd.DataFrame.from_dict(records_dict, orient=&#39;index&#39;)
        df.to_csv(self.csv_outfile, index=False)

        # Write arxiv_id list
        arxiv_outfile = self.csv_outfile.replace(&#39;.csv&#39;, &#39;_arxiv.txt&#39;)
        clean_df = df.loc[(df[&#39;arxiv_id&#39;] != &#39;&#39;) &amp;amp; (df[&#39;arxiv_id&#39;].notna())]
        clean_df.to_csv(arxiv_outfile, index=False, header=False,
                        columns=[&#39;arxiv_id&#39;])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As you can see, two types of files are provided: JSON and
CSV. The latter is done with &lt;a href=&#34;https://pandas.pydata.org/&#34;&gt;&lt;code&gt;pandas&lt;/code&gt;&lt;/a&gt;.
An example of the exported JSON output is available below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;voxcharta_json.jpg&#34; &gt;
&lt;img src=&#34;voxcharta_json.jpg&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;I then provided my voting records to the maintainers of the
&lt;a href=&#34;https://www.benty-fields.com/&#34;&gt;Benty-Fields&lt;/a&gt; web application. It
has many features that are similar to Vox Charta by allowing for
discussions and voting. In the end, those records were transferred
over!&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;benty_fields.png&#34; &gt;
&lt;img src=&#34;benty_fields.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;If you&#39;re interested in using this code, it is available
&lt;a href=&#34;https://github.com/astrochun/voxcharta-my-voting-record&#34;&gt;here&lt;/a&gt;.
The code is made available under a
&lt;a href=&#34;https://github.com/astrochun/voxcharta-my-voting-record/blob/main/LICENSE&#34;&gt;GNU GPL-3.0&lt;/a&gt;
License so other astronomers can preserve their precious Vox Charta records.&lt;/p&gt;
&lt;p&gt;The README.md provides more details, but here are some quick
installation and execution steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Download the complete webpage view from the &amp;ldquo;My Voting Record&amp;rdquo; page before
the site is permanently down&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Clone the repo:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/astrochun/voxcharta-my-voting-record
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Build it (preferably in a contained environment)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ cd voxcharta-my-voting-record
$ (sudo) python setup.py install
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can then easily execute the main script:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ vox_run -f myvotingrecords.htm
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Many thanks to James Guillochon for being the selfless maintainer of Vox
Charta for over a decade. I&#39;m also thankful to the Benty-Fields maintainers
for their help.&lt;/p&gt;
&lt;p&gt;If you found this &lt;strong&gt;open-source&lt;/strong&gt; software useful, consider sponsoring me
through GitHub.&lt;/p&gt;
&lt;iframe src=&#34;https://github.com/sponsors/astrochun/button&#34;
title=&#34;Sponsor astrochun&#34; height=&#34;35&#34; width=&#34;116&#34; style=&#34;border: 0;&#34;&gt;
&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>When the pandemic hit, I used my data analysis and Google Sheet skills to inform</title>
      <link>https://astrochun.github.io/post/covid19_research/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/covid19_research/</guid>
      <description>&lt;p&gt;Without a doubt, COVID-19 has impacted every aspect of life, especially higher
education. Earlier this year, the University of Arizona (UofA)
&lt;a href=&#34;https://view.comms.arizona.edu/?qs=af589cd97cb2617e9fdf2ab657a0d6345f451dbc795ba254bc81195f60626149c14bc1d2f83064236848f4247e26ebfa57a0c9858c2d3201e12beef6136f1cf0df93fc33ec93b71912398904fbc91c6b&#34;&gt;announced&lt;/a&gt;
it would start the Fall 2020 semester in a hybrid mode of in-person and online
instruction, and had prepared for this re-entry in a number of ways.
One of those priorities is testing of students, staff, and faculty to identify
active cases. The UofA was not alone, as many other universities had implemented
their own COVID-19 testing program.&lt;/p&gt;
&lt;p&gt;In late August, the UofA provided summary data containing new daily tests
(for the previous day) and cumulative numbers (July 31&amp;ndash;) on their main
&lt;a href=&#34;https://covid19.arizona.edu/updates&#34;&gt;COVID-19 Updates webpage&lt;/a&gt;.  Over time, it
was clear that the numbers were not improving, but I grew frustrated as it was
difficult to see &lt;em&gt;quantitative&lt;/em&gt; trends.  Prior to the start of the semester, it
was said during a COVID-19 webinar that the
&lt;a href=&#34;https://covid19.arizona.edu/dashboard&#34;&gt;COVID-19 dashboard&lt;/a&gt; would be made
available in late August.  So on September 3, I decided to start an independent
volunteer project where I started to aggregate the summary data.  I announced
it on &lt;a href=&#34;https://twitter.com/astrochunly/status/1301532415773478912&#34;&gt;Twitter&lt;/a&gt;, and
started with whatever screenshots I had saved and began putting the information
in a &lt;a href=&#34;https://bit.ly/UArizona_COVID_Testing&#34;&gt;Google Sheet&lt;/a&gt;, which I made &lt;em&gt;publicly&lt;/em&gt;
available for &lt;strong&gt;everyone&lt;/strong&gt; to see. Over time, I had the help of a few UofA colleagues
that connected with me through social media:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Jill McCleary, Deputy Director and Acting Head at &lt;a href=&#34;https://artmuseum.arizona.edu/&#34;&gt;UofA Museum of Art&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/ironyofprint&#34;&gt;Cheryl Knott&lt;/a&gt;, Professor in the School of Information&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/scastiello&#34;&gt;Santiago Castiello-Gutiérrez&lt;/a&gt;, Ph.D. candidate in the Higher Education program&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I am so thankful to them as this project would not have been possible without them.&lt;/p&gt;
&lt;p&gt;I chose Google Sheet for many reasons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Easy to use&lt;/li&gt;
&lt;li&gt;Easy to publish&lt;/li&gt;
&lt;li&gt;Easy to collaborate&lt;/li&gt;
&lt;li&gt;Easy to make charts/graphs&lt;/li&gt;
&lt;li&gt;As a programmer, built-in version control&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This project was very much &lt;a href=&#34;https://en.wikipedia.org/wiki/Open_science&#34;&gt;#openscience&lt;/a&gt;
as hundreds of users checked out the spreadsheet daily to look at the latest updates.
Since many users viewed it anonymously, it was hard to get exact numbers, but this
was what I found from Twitter, Google, and Bitly tracking.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I estimate a total of 545 clicks on either the original Google or Bitly link&lt;/li&gt;
&lt;li&gt;393 (72%) of those clicks were through Twitter and Facebook&lt;/li&gt;
&lt;li&gt;The spreadsheet was shared via other platforms (SMS, email, etc.) for nearly
152 additional clicks (28%)&lt;/li&gt;
&lt;li&gt;There were 146 unique non-anonymous viewers to the Google Sheet&lt;/li&gt;
&lt;li&gt;Using Bitly, I was able to see that viewers included those in at least ten
different foreign countries.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This all happened mostly during a nearly two-week period before the COVID-19 dashboard
went live on September 16.&lt;/p&gt;
&lt;p&gt;The source of information came from my colleagues,
&lt;a href=&#34;https://www.youtube.com/c/universityofarizona&#34;&gt;UofA&#39;s weekly re-entry broadcasting&lt;/a&gt;,
and using the &lt;a href=&#34;https://archive.org/web/&#34;&gt;WayBackMachine&lt;/a&gt; to go back and look at the
websites. After all the data were aggregated, there was a lot of quality control as some
numbers disagreed, and that needed to be recorded in the Google Sheet with
in-cell comments and source identification.  This is what we call good
&lt;a href=&#34;https://en.wikipedia.org/wiki/Data_management&#34;&gt;data management&lt;/a&gt; practice.
A preview of the machine-readable consolidated spreadsheet is available below:&lt;/p&gt;
&lt;p&gt;&lt;iframe src=&#34;https://docs.google.com/spreadsheets/d/e/2PACX-1vQ3asdKKUNbQj9tpMo8dircaSVxDHJR0fKo8L9cLQ3TMqHb-KBzHmCT_iRl2ie6xbtHEV3V5EHZjuo-/pubhtml?gid=880188181&amp;single=true&#34; frameborder=&#34;0&#34; width=&#34;715&#34; height=&#34;450&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;Then, after I felt confident of all the numbers, &lt;a href=&#34;featured.png&#34;&gt;I put together graphs that summarized the testing data.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The final culmination of this hard work by my colleagues and I is publishing
the final table and visualization in the
&lt;a href=&#34;https://arizona.figshare.com&#34;&gt;UofA Research Data Repository&lt;/a&gt;. The work is titled
&lt;a href=&#34;https://doi.org/10.25422/azu.data.12966581&#34;&gt;&amp;ldquo;Independent Data Aggregation, Quality Control and Visualization of University of Arizona COVID-19 Re-Entry Testing Data&amp;rdquo;&lt;/a&gt; and is made available
under a &lt;a href=&#34;https://creativecommons.org/publicdomain/zero/1.0/deed.en&#34;&gt;Creative Commons Zero (CC0)&lt;/a&gt;
public domain license:&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;&lt;iframe src=&#34;https://widgets.figshare.com/articles/12966581/embed?show_title=true&#34; width=&#34;568&#34; height=&#34;351&#34; allowfullscreen frameborder=&#34;0&#34;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;h2 id=&#34;final-thoughts&#34;&gt;Final Thoughts&lt;/h2&gt;
&lt;p&gt;I really enjoyed being able to share what I learned with the UofA community.
I found myself needing to make this happen so everyone knew how fast COVID-19
was spreading in the UofA community. We &lt;strong&gt;all&lt;/strong&gt; need to pull together to
fight back, and gathering, checking and illustrating information was the best
way for me to go about it.&lt;/p&gt;
&lt;p&gt;#StaySafe, #BearDown, and #MaskUp!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Syncing an Overleaf project with your GitHub account</title>
      <link>https://astrochun.github.io/post/overleaf_github_git_sync/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/post/overleaf_github_git_sync/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.overleaf.com&#34;&gt;Overleaf&lt;/a&gt; is an environment for researchers to
collaboratively write their manuscripts and papers with $\LaTeX$. It is
an amazing tool, but I have found that it has its limitations. For example,
if you edit within their web UI, the changes may show up as separate commits
in their History page.  One of the benefits of Overleaf is that it is
integrated with &lt;code&gt;git&lt;/code&gt;.  This is great as you can maintain version control
and document changes to your collaborators.&lt;/p&gt;
&lt;p&gt;With the power of &lt;code&gt;git&lt;/code&gt;, I wanted to find a way to bring all the version
control to &lt;a href=&#34;https://www.github.com&#34;&gt;GitHub&lt;/a&gt; and their nice UI to illustrate
changes.&lt;/p&gt;
&lt;p&gt;I came across this &lt;a href=&#34;https://ineed.coffee/3454/how-to-synchronize-an-overleaf-latex-paper-with-a-github-repository/&#34;&gt;tutorial&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;The trick is using the default &lt;code&gt;git remote&lt;/code&gt; command.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1&lt;/strong&gt;: Sign up for a free account at &lt;a href=&#34;https://www.overleaf.com&#34;&gt;Overleaf&lt;/a&gt;
and create a new project (I recommend the &amp;ldquo;Example Project&amp;rdquo; option):&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;new_project.png&#34; data-caption=&#34;Step 1: Create a new Overleaf project&#34;&gt;
&lt;img src=&#34;new_project.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 1: Create a new Overleaf project
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Step 2&lt;/strong&gt;: Retrieve the &lt;code&gt;git clone&lt;/code&gt; command by selecting the Menu button in the
upper left and the &lt;code&gt;Git&lt;/code&gt; option:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;git_clone.png&#34; data-caption=&#34;Step 2: Retrieve git clone command&#34;&gt;
&lt;img src=&#34;git_clone.png&#34; alt=&#34;&#34; width=&#34;50%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 2: Retrieve &lt;code&gt;git clone&lt;/code&gt; command
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;You should see a pop-up window containing something similar to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;               git clone https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here I use &lt;code&gt;&amp;lt;ABCDEF123456&amp;gt;&lt;/code&gt; as a placeholder for the real path throughout this post.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3&lt;/strong&gt;: Open up a command-line terminal and execute this &lt;code&gt;git clone&lt;/code&gt; command and
finish it with a more meaningful project name:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git clone https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; git_integration_project
$ cd git_integration_project
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You now have a copy of your Overleaf project locally! You can open the content
with your favorite text editor, and &lt;code&gt;git add&lt;/code&gt;, &lt;code&gt;git commit&lt;/code&gt;, and &lt;code&gt;git push&lt;/code&gt;
new changes to Overleaf.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Step 4&lt;/strong&gt;: To take this one step further and mirror a copy on GitHub,
let&#39;s create an &lt;strong&gt;empty&lt;/strong&gt; &lt;a href=&#34;https://github.com/new&#34;&gt;new GitHub repository&lt;/a&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;github_new.png&#34; data-caption=&#34;Step 4: Create a new empty GitHub repository&#34;&gt;
&lt;img src=&#34;github_new.png&#34; alt=&#34;&#34; width=&#34;75%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Step 4: Create a new empty GitHub repository
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Note that this is entirely empty. This is very important as the content
will be populated from your Overleaf repository! Also, I made it a &lt;em&gt;private&lt;/em&gt;
repository. You might find that important as well since you might be
working on a paper that you want to keep private.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 5&lt;/strong&gt;: Within the &lt;code&gt;git_integration project&lt;/code&gt; folder, create a new &lt;code&gt;git remote&lt;/code&gt;
called &lt;code&gt;github&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote add github https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let&#39;s look at our &lt;code&gt;git remote -v&lt;/code&gt; setup:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote -v
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (fetch)
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (push)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (fetch)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (push)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Step 6&lt;/strong&gt;: Let us now replicate what we have on Overleaf to GitHub:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git push github (master)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you refresh your GitHub repository, you should see all the $\LaTeX$ files.&lt;/p&gt;
&lt;p&gt;Of course, it would be nice to be able to push to both git repositories instead
of having to do:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git push origin master
$ git push github master
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Step 7&lt;/strong&gt;: To do this, let&#39;s create another &lt;code&gt;git remote&lt;/code&gt; called &lt;code&gt;both&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote add both https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt;
$ git remote set-url --add --push both https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt;
$ git remote set-url --add --push both https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us inspect the &lt;code&gt;git remote -v&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ git remote -v
both	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (fetch)
both	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (push)
both	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (push)
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (fetch)
github	https://github.com/&amp;lt;USERNAME&amp;gt;/git_integration_project.git (push)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (fetch)
origin	https://git.overleaf.com/&amp;lt;ABCDEF123456&amp;gt; (push)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And that&#39;s it! Now, any &lt;code&gt;git push both (master)&lt;/code&gt; will update both Overleaf and
GitHub.&lt;/p&gt;
&lt;p&gt;Now $\LaTeX$ + &lt;code&gt;git push&lt;/code&gt; on!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>UofA Research Data Repository</title>
      <link>https://astrochun.github.io/project/redata/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/redata/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;There has been a growing needs for data publishing and digital preservation to
enable open and reproducible science.  One of the major drivers for researchers
is the requirement from federal agencies (e.g., NSF, NASA, NIH, DoE)
of a &lt;a href=&#34;https://data.library.arizona.edu/data-management-plans&#34;&gt;data management plan&lt;/a&gt;.
Data repositories, such as, &lt;a href=&#34;https://dataverse.org/&#34;&gt;Dataverse&lt;/a&gt;,
&lt;a href=&#34;https://zenodo.org/&#34;&gt;Zenodo&lt;/a&gt;, and &lt;a href=&#34;https://figshare.com/&#34;&gt;Figshare&lt;/a&gt; are being
used across all disciplines, including science, humanities, medicine, and
technology to satisfy funder&#39;s requirements and other needs (e.g., publishers
requirements). These repository services focus on following the guiding data
principles of &lt;a href=&#34;https://www.go-fair.org/fair-principles/&#34;&gt;FAIR&lt;/a&gt;
(Findable, Accessible, Interoperable, and Reusable).&lt;/p&gt;
&lt;p&gt;In addition to these public repositories, R-1 universities have been providing
institutional data repositories to serve the greater needs of their university.
These repositories also enable university administration to track
research output and productivity.&lt;/p&gt;
&lt;p&gt;Motivated by a number of factors, the &lt;a href=&#34;https://new.libary.arizona.edu&#34;&gt;University of Arizona Libraries&lt;/a&gt;
is leading the effort to develop and sustain a data repository with funding from the
&lt;a href=&#34;https://provost.arizona.edu/provost-investment-fund&#34;&gt;Provost Investment Fund&lt;/a&gt;.
The repository, called ReDATA, will be hosted through an
&lt;a href=&#34;https://knowledge.figshare.com/institutions&#34;&gt;institutional Figshare&lt;/a&gt; site.&lt;/p&gt;
&lt;p&gt;More information about ReDATA is available
&lt;a href=&#34;https://data.library.arizona.edu/services/ua-research-data-repository&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In my current role as Research Data Systems Integration Specialist, I:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop front- and/or back-end software to enable and automate
identity and access management, data curation, and data preservation,
and deploying them on cloud infrastructure&lt;/li&gt;
&lt;li&gt;Engage with major campus stakeholders to identify and
develop workflows for integrating ReDATA with existing library, campus
computing, and research administration systems, such as, single-sign on
(SSO) with the University&#39;s enterprise-level Shibboleth system,
&lt;a href=&#34;https://orcid.org/&#34;&gt;ORCiD&lt;/a&gt;, and &lt;a href=&#34;http://hpc.arizona.edu/&#34;&gt;UofA HPC&lt;/a&gt;
infrastructure&lt;/li&gt;
&lt;li&gt;Engage with researchers across campus to use the data
repository for open and reproducible science&lt;/li&gt;
&lt;li&gt;Participate in discussions regarding data repository
policies.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;announcements&#34;&gt;Announcements&lt;/h3&gt;
&lt;h4 id=&#34;seeking-aditional-deposits-in-our-closed-beta-phase&#34;&gt;Seeking aditional deposits in our closed beta phase&lt;/h4&gt;
&lt;p&gt;[2020-06-01] Access to ReDATA service is currently by request only.
Please contact us at &lt;a href=&#34;data-management@arizona.edu&#34;&gt;&lt;a href=&#34;mailto:data-management@arizona.edu&#34;&gt;data-management@arizona.edu&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;service-is-publicly-available-seeking-early-adopters&#34;&gt;Service is publicly available. Seeking Early Adopters&lt;/h4&gt;
&lt;p&gt;[2020-03-01] We are looking for early adopters to deposit research
datasets, code, media, and other materials and to provide feedback on process
and policies, while acting as champions for the repository.&lt;/p&gt;
&lt;p&gt;If you’d like to influence how UA’s public research data repository will
operate, we would like to work with you! As an early adopter, you’ll be able
to work directly with the data repository team.  Please contact us at:
&lt;a href=&#34;data-management@arizona.edu&#34;&gt;&lt;a href=&#34;mailto:data-management@arizona.edu&#34;&gt;data-management@arizona.edu&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;upcoming-talks-and-events&#34;&gt;Upcoming Talks and Events&lt;/h3&gt;
&lt;p&gt;We are planning a number of events in 2020.  Events will include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Weekly drop-in for help with depositing data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This page is under construction. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tech Talks</title>
      <link>https://astrochun.github.io/project/techtalks/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/techtalks/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Below are materials for recent presentations and blog posts:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://hackmd.io/NP65F9wUQiCC_kpPFrSivA?view&#34;&gt;Research Bazaar Tucson 2020: Beautiful Technical Documents Easily Made with $\LaTeX$ + Overleaf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://astrochun.github.io/post/overleaf_github_git_sync&#34;&gt;Syncing an Overleaf project with your GitHub account&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The stellar population of metal-poor galaxies at z ensuremath≈ 0.8 and the evolution of the mass-metallicity relation</title>
      <link>https://astrochun.github.io/publication/2020-mnras-491-2254-w/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2020-mnras-491-2254-w/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NewH$\alpha$ Survey</title>
      <link>https://astrochun.github.io/project/newha/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/newha/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This page is under construction.  Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gemini GNIRS Longslit Reduction</title>
      <link>https://astrochun.github.io/project/gnirslongslit/</link>
      <pubDate>Mon, 23 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/gnirslongslit/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;https://www.gemini.edu/sciops/instruments/gnirs/&#34;&gt;Gemini Near-Infrared Spectograph (GNIRS)&lt;/a&gt;
was commissioned in 2004 by the &lt;a href=&#34;https://www.gemini.edu/&#34;&gt;Gemini Observatory&lt;/a&gt;.
With coverage of 1&amp;ndash;5$\mu$m, it remains a work-horse facility instrument on
the Gemini North telescope in Hawai`i.&lt;/p&gt;
&lt;p&gt;Recently, I used this instrument to obtain near-infrared spectra for low-mass
galaxies at $z\approx0.8$. I learned that while Gemini has provided an &lt;a href=&#34;https://www.gemini.edu/node/11823&#34;&gt;IRAF
reduction pipeline&lt;/a&gt;, automated
implementation of this pipeline has been limited. For example, this
&lt;a href=&#34;https://www.gemini.edu/sciops/data-and-results/getting-started#gnirs&#34;&gt;website&lt;/a&gt;
provided a number of tutorials, but ultimately the user needed to create scripts
to execute, as this &lt;a href=&#34;http://www.gemini.edu/sciops/data/IRAFdoc/gnirs_xd_example.cl&#34;&gt;example script&lt;/a&gt;
demonstrates.  While Gemini recently released the &lt;a href=&#34;https://ascl.net/1811.002&#34;&gt;DRAGONS Python-based
data reduction pipeline&lt;/a&gt;, reduction is limited
to imaging data.&lt;/p&gt;
&lt;h2 id=&#34;the-automation-pipeline&#34;&gt;The Automation Pipeline&lt;/h2&gt;
&lt;p&gt;With spectra taken on several dozens of nights, automation was critical to
expedite data reduction for my project. Thus, I created a Python/PyRAF
pipeline that used the existing Gemini IRAF package. This automation pipeline
is made publicly available on &lt;a href=&#34;https://github.com/astrochun/GNIRSLongSlit&#34;&gt;GitHub&lt;/a&gt;
and distributed under an &lt;a href=&#34;https://choosealicense.com/licenses/mit/&#34;&gt;MIT License&lt;/a&gt;.
Because of the dependence on PyRAF/IRAF, this code is only compatible with
Python 2.7.&lt;/p&gt;
&lt;p&gt;The pipeline has several features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Longslit target acquisition checks with quality assurance (QA) visualization&lt;/li&gt;
&lt;li&gt;Correct detector readout artefacts with
&lt;a href=&#34;https://www.gemini.edu/sciops/instruments/niri/data-format-and-reduction/cleanir&#34;&gt;&lt;code&gt;CLEANIR&lt;/code&gt; routine&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Flat fielding&lt;/li&gt;
&lt;li&gt;Wavelength calibration&lt;/li&gt;
&lt;li&gt;Dithered stacking&lt;/li&gt;
&lt;li&gt;Telluric flux calibration&lt;/li&gt;
&lt;li&gt;Spectral extraction&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;Installation is best done by cloning the GitHub repository and installing PyRAF.&lt;/p&gt;
&lt;p&gt;This page is under construction. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Subaru Deep Field (SDF)</title>
      <link>https://astrochun.github.io/project/sdf/</link>
      <pubDate>Sat, 21 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/sdf/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The Subaru 8.2-meter telescope has stared for over 30 nights at one particular
region of the northern sky with one of the most sensitive optical imagers in
the world (Suprime-Cam) to develop the Subaru Deep Field (SDF;
&lt;a href=&#34;https://dx.doi.org/:10.1093/pasj/56.6.1011&#34;&gt;Kashikawa et al. 2004&lt;/a&gt;). This camera
enables images to be taken over a large part of the sky that is comparable to
the angular extent of the Moon. The large area allows us to look at hundreds
of thousands of galaxies simultaneously.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://astrochun.github.io/img/sdf_moon.png&#34; data-caption=&#34;Areal comparison of the SDF against the Hubble Deep Field (HDF) and the Moon&#34;&gt;
&lt;img src=&#34;https://astrochun.github.io/img/sdf_moon.png&#34; alt=&#34;&#34; width=&#34;65%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Areal comparison of the SDF against the Hubble Deep Field (HDF) and the Moon
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Why stare for so long? To identify some of the faintest galaxies in the early
universe! To do so, the survey utilized a unique technique called &amp;ldquo;narrowband
imaging&amp;rdquo; to pick up emission from hydrogen and oxygen gas that reside within
galaxies.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://astrochun.github.io/img/Ly2014_fig1.jpg&#34; data-caption=&#34;Narrowband and intermediate filters for the SDF (adapted from Ly et al. 2014)&#34;&gt;
&lt;img src=&#34;https://astrochun.github.io/img/Ly2014_fig1.jpg&#34; alt=&#34;&#34; width=&#34;75%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Narrowband and intermediate filters for the SDF (adapted from &lt;a href=&#34;http://dx.doi.org/10.1088/0004-637X/780/2/122&#34;&gt;Ly et al. 2014&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;i-luminosity-function-and-star-formation-rate-density&#34;&gt;I. Luminosity Function and Star Formation Rate Density&lt;/h3&gt;
&lt;h3 id=&#34;ii-lyman-break-galaxies-in-the-redshift-desert-with-galex-imaging&#34;&gt;II. Lyman Break Galaxies in the Redshift Desert with &lt;em&gt;GALEX&lt;/em&gt; imaging&lt;/h3&gt;
&lt;h3 id=&#34;iii-census-of-starforming-galaxies-at-galaxy-high-noon&#34;&gt;III. Census of Star-Forming Galaxies at &amp;ldquo;Galaxy High Noon&amp;rdquo;&lt;/h3&gt;
&lt;h3 id=&#34;iv-dust-attenuation-and-star-formation-in-dwarf-galaxies&#34;&gt;IV. Dust Attenuation and Star Formation in Dwarf Galaxies&lt;/h3&gt;
&lt;h3 id=&#34;v&#34;&gt;V.&lt;/h3&gt;
&lt;h3 id=&#34;vi-extremely-metalpoor-galaxies&#34;&gt;VI. Extremely Metal-Poor Galaxies&lt;/h3&gt;
&lt;p&gt;Using the MMT, Keck, and Subaru observatories, my collaborators and I have
targeted some of these faint galaxies with optical and near-infrared
spectrographs. We found a rare population of galaxies that are rapidly forming
at an unexpected rate. Many of these galaxies show signs of interactions with
other nearby galaxies (see below). We believe that these interactions trigger
the extreme bursts of star formation that we see. With detections of a very
weak line, [OIII]λ4363, we were able to determine that these galaxies do not
have significant heavy elements. In fact, a few of our galaxies have the most
metal pristine gas seen for galaxies outside the local Universe. I am trying to
understand why these galaxies have so few metals. One interpretation is that
these galaxy interactions can drive new metal pristine gas into the galaxies,
\making it appear &amp;ldquo;metal-poor&amp;rdquo; and fueling recent star formation. You can
find more information in our paper.&lt;/p&gt;
&lt;h3 id=&#34;vii&#34;&gt;VII.&lt;/h3&gt;
&lt;p&gt;This page is under construction. Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Automation of MMT/MMIRS data reduction</title>
      <link>https://astrochun.github.io/project/mmttools/mmirs/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/mmttools/mmirs/</guid>
      <description>&lt;p&gt;Reduction of &lt;a href=&#34;https://www.cfa.harvard.edu/mmti/mmirs.html&#34;&gt;MMIRS (MMT and Magellan Infrared Spectograph)&lt;/a&gt;
spectral data is possible using an existing open-source pipeline called
&lt;code&gt;mmirs-pipeline&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The pipeline software, developed by Igor Chilingarian (SAO), is published
in the &lt;a href=&#34;http://dx.doi.org/10.1086/680598&#34;&gt;PASP Journal&lt;/a&gt;. The pipeline&#39;s
git repository is available &lt;a href=&#34;https://bitbucket.org/chil_sai/mmirs-pipeline/wiki/Home&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One difficulty with using &lt;code&gt;mmirs-pipeline&lt;/code&gt; is the use of &amp;ldquo;taskfiles&amp;rdquo; that
describe the observation and the steps for data reduction.  These files are
constructed for each pair of dithered spectra.  No existing software existed to
create these files.  As such, a lot of manual effort on the part of a researcher
was needed.&lt;/p&gt;
&lt;h2 id=&#34;the-python-tool&#34;&gt;The Python Tool&lt;/h2&gt;
&lt;p&gt;As part of a larger Python git repository (&lt;a href=&#34;www.github.com/astrochun/MMTtools&#34;&gt;MMTtools&lt;/a&gt;),
&lt;code&gt;mmirs_pipeline_taskfile&lt;/code&gt; is a Python tool (compatible with 2.7 and 3.xx) that
constructs the necessary task files and IDL script for execution. This code is
made publicly available under an &lt;a href=&#34;https://choosealicense.com/licenses/mit/&#34;&gt;MIT License&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;installation&#34;&gt;Installation&lt;/h3&gt;
&lt;p&gt;The code is available through PyPI, although cloning from the GitHub repository
will give you the latest stable version.  Below are the steps for installation and the
necessary prerequisite:&lt;/p&gt;
&lt;p&gt;This page is still under construction.  Stay tuned!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>DEEP2 Survey</title>
      <link>https://astrochun.github.io/project/deep2/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/deep2/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The &lt;a href=&#34;http://deep.ps.uci.edu/&#34;&gt;DEEP2 Survey&lt;/a&gt; is an extragalactic survey conducted
by the University of California to obtain deep optical spectra for $\sim$50,000 faint
galaxies at redshifts of $0 &amp;lt; z &amp;lt; 1.4$. These spectra have provided accurate
redshifts for $\approx$40,000 galaxies. While the spectra were primarily used
to determine distances/redshifts using two emission lines, a subset of the
sample (4,140 galaxies) contains spectral coverage that span 3700&amp;ndash;5010 Angstroms,
enabling gas-phase chemical abundance determination.&lt;/p&gt;
&lt;p&gt;Using the &lt;a href=&#34;http://deep.ps.uci.edu/DR4/home.html&#34;&gt;publicly available data&lt;/a&gt;, I
identified a large sample of 28 galaxies with detection of
$[{\rm O~{\small III}}]\lambda4363$, a gas temperature diagnostics.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://astrochun.github.io/img/Ly2015_fig1.jpg&#34; data-caption=&#34;Illustration of eight galaxies with $[{\rm O~{\small III}}]\lambda4363$ detections (Ly et al. 2015)&#34;&gt;
&lt;img src=&#34;https://astrochun.github.io/img/Ly2015_fig1.jpg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Illustration of eight galaxies with $[{\rm O~{\small III}}]\lambda4363$ detections (&lt;a href=&#34;https://dx.doi.org/10.1088/0004-637X/805/1/45&#34;&gt;Ly et al. 2015&lt;/a&gt;)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This work, titled &amp;ldquo;&lt;em&gt;Metal-poor, Strongly Star-forming Galaxies in the DEEP2 Survey:
The Relationship between Stellar Mass, Temperature-based Metallicity, and Star
Formation Rate&lt;/em&gt;&amp;quot;, is published in the
&lt;a href=&#34;https://dx.doi.org/10.1088/0004-637X/805/1/45&#34;&gt;&lt;em&gt;Astrophysical Journal&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;spitzer-followup-imaging-to-characterize-stellar-population&#34;&gt;&lt;em&gt;Spitzer&lt;/em&gt; follow-up imaging to characterize stellar population&lt;/h2&gt;
&lt;p&gt;While the spectra enabled us to characterize the interstellar gas properties
of these metal-poor galaxies, understanding the stellar properties of these
$[{\rm O~{\small III}}]\lambda4363$-detected galaxies requires observations
in the infrared to detect the light from low-mass stars.  Using the
&lt;a href=&#34;https://www.cfa.harvard.edu/irac/&#34;&gt;InfraRed Array Camera (IRAC)&lt;/a&gt; on the
NASA&#39;s &lt;a href=&#34;http://www.spitzer.caltech.edu/&#34;&gt;&lt;em&gt;Spitzer Space Telescope&lt;/em&gt;&lt;/a&gt;, I
obtained deep imaging at 3.6 $\mu$m.&lt;/p&gt;



  











&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://astrochun.github.io/img/Weldon19_fig1.jpg&#34; data-caption=&#34;Spitzer/IRAC observations of all 28 metal-poor galaxies from Ly et al. (2015)&#34;&gt;
&lt;img src=&#34;https://astrochun.github.io/img/Weldon19_fig1.jpg&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;em&gt;Spitzer&lt;/em&gt;/IRAC observations of all 28 metal-poor galaxies from Ly et al. (2015)
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;These observations demonstrated that these galaxies are very faint, and can
be classified as &amp;ldquo;dwarf&amp;rdquo; galaxies. In addition, our work illustrated a strong
evolution in the mass&amp;ndash;metallicity relation, $(1+z)^{-1.45^{+0.61}_{-0.76}}$.
Specifically, we find that lower mass galaxies ($4\times10^8~M_{\odot}$) built
up their metal content 1.6 times more rapidly than higher mass galaxies
($10^{10}~M_{\odot}$). We also found that there is not a secondary
dependence on star formation in the mass&amp;ndash;metallicity relation.&lt;/p&gt;
&lt;p&gt;This work is now published in the
&lt;a href=&#34;http://dx.doi.org/10.1093/mnras/stz3047&#34;&gt;&lt;em&gt;Monthly Notices of the Royal Astronomical Society&lt;/em&gt;&lt;/a&gt;
as a first-author paper for Drew Weldon, a former undergraduate astronomy major at the
UofA and now attending UC Riverside for graduate school.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Metal Abundances across Cosmic Time</title>
      <link>https://astrochun.github.io/project/mact/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/project/mact/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h2 id=&#34;evolution-of-the-massmetallicity-relation&#34;&gt;Evolution of the Mass&amp;ndash;Metallicity Relation&lt;/h2&gt;
&lt;h2 id=&#34;main-sequence-relation-of-extremely-lowmass-galaxies&#34;&gt;Main Sequence Relation of Extremely Low-Mass Galaxies&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>Mapping the Stellar Halo with the H3 Spectroscopic Survey</title>
      <link>https://astrochun.github.io/publication/2019-ap-j-883-107-c/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2019-ap-j-883-107-c/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Follow-up of the Neutron Star Bearing Gravitational-wave Candidate Events S190425z and S190426c with MMT and SOAR</title>
      <link>https://astrochun.github.io/publication/2019-ap-j-880-l-4-h/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2019-ap-j-880-l-4-h/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Structure and Dynamics of the Subparsec Jet in M87 Based on 50 VLBA Observations over 17 Years at 43 GHz</title>
      <link>https://astrochun.github.io/publication/2018-ap-j-855-128-w/</link>
      <pubDate>Thu, 01 Mar 2018 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2018-ap-j-855-128-w/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lyman-break Galaxies at z tilde 3 in the Subaru Deep Field: Luminosity Function, Clustering, and [O III] Emission</title>
      <link>https://astrochun.github.io/publication/2017-ap-j-850-5-m/</link>
      <pubDate>Wed, 01 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2017-ap-j-850-5-m/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Observations of the Structure and Dynamics of the Inner M87 Jet</title>
      <link>https://astrochun.github.io/publication/2016-galax-4-46-w/</link>
      <pubDate>Sat, 01 Oct 2016 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2016-galax-4-46-w/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Metal Abundances across Cosmic Time (MACT) Survey. I. Optical Spectroscopy in the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2016-ap-js-226-5-l/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2016-ap-js-226-5-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Metal Abundances across Cosmic Time (MACT) Survey. II. Evolution of the Mass-metallicity Relation over 8 Billion Years, Using [OIII]4363AA-based Metallicities</title>
      <link>https://astrochun.github.io/publication/2016-ap-j-828-67-l/</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2016-ap-j-828-67-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Physical conditions of the interstellar medium in star-forming galaxies at z tilde 1.5</title>
      <link>https://astrochun.github.io/publication/2015-pasj-67-80-h/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2015-pasj-67-80-h/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Metal-poor, Strongly Star-forming Galaxies in the DEEP2 Survey: The Relationship between Stellar Mass, Temperature-based Metallicity, and Star Formation Rate</title>
      <link>https://astrochun.github.io/publication/2015-ap-j-805-45-l/</link>
      <pubDate>Fri, 01 May 2015 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2015-ap-j-805-45-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Relationship between Stellar Mass, Gas Metallicity, and Star Formation Rate for H$α$-Selected Galaxies at z ap 0.8 from the NewH$α$ Survey</title>
      <link>https://astrochun.github.io/publication/2015-aj-149-79-d/</link>
      <pubDate>Sun, 01 Feb 2015 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2015-aj-149-79-d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Critical Look at the Mass-Metallicity-Star Formation Rate Relation in the Local Universe. I. An Improved Analysis Framework and Confounding Systematics</title>
      <link>https://astrochun.github.io/publication/2014-ap-j-797-126-s/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2014-ap-j-797-126-s/</guid>
      <description></description>
    </item>
    
    <item>
      <title>``Direct&#39;&#39; Gas-phase Metallicities, Stellar Properties, and Local Environments of Emission-line Galaxies at Redshifts below 0.90</title>
      <link>https://astrochun.github.io/publication/2014-ap-j-780-122-l/</link>
      <pubDate>Wed, 01 Jan 2014 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2014-ap-j-780-122-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emission-line Galaxies from the Hubble Space Telescope Probing Evolution and Reionization Spectroscopically (PEARS) Grism Survey. II. The Complete Sample</title>
      <link>https://astrochun.github.io/publication/2013-ap-j-772-48-p/</link>
      <pubDate>Mon, 01 Jul 2013 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2013-ap-j-772-48-p/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Nebular Attenuation in H$α$-selected Star-forming Galaxies at z = 0.8 from the NewH$α$ Survey</title>
      <link>https://astrochun.github.io/publication/2013-aj-145-47-m/</link>
      <pubDate>Fri, 01 Feb 2013 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2013-aj-145-47-m/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Ly$α$ Emitter with an Extremely Large Rest-frame Equivalent Width of ̃900 Å at z = 6.5: A Candidate Population III-dominated Galaxy?</title>
      <link>https://astrochun.github.io/publication/2012-ap-j-761-85-k/</link>
      <pubDate>Sat, 01 Dec 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-ap-j-761-85-k/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unusual Long and Luminous Optical Transient in the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2012-ap-j-760-l-11-u/</link>
      <pubDate>Thu, 01 Nov 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-ap-j-760-l-11-u/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Stellar Population and Star Formation Rates of z ap 1.5-1.6 [O II]-emitting Galaxies Selected from Narrowband Emission-line Surveys</title>
      <link>https://astrochun.github.io/publication/2012-ap-j-757-63-l/</link>
      <pubDate>Sat, 01 Sep 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-ap-j-757-63-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Dual-Narrowband Survey for H$α$ Emitters at Redshift of 2.2: Demonstration of the Technique and Constraints on the H$α$ Luminosity Function</title>
      <link>https://astrochun.github.io/publication/2012-pasp-124-782-l/</link>
      <pubDate>Sun, 01 Jul 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-pasp-124-782-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Dust Attenuation and H$α$ Star Formation Rates of z ̃ 0.5 Galaxies</title>
      <link>https://astrochun.github.io/publication/2012-ap-j-747-l-16-l/</link>
      <pubDate>Thu, 01 Mar 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-ap-j-747-l-16-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The 2010 Very High Energy $γ$-Ray Flare and 10 Years of Multi-wavelength Observations of M 87</title>
      <link>https://astrochun.github.io/publication/2012-ap-j-746-151-a/</link>
      <pubDate>Wed, 01 Feb 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-ap-j-746-151-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Average Metallicity and Star Formation Rate of Ly$α$ Emitters Probed by a Triple Narrowband Survey</title>
      <link>https://astrochun.github.io/publication/2012-ap-j-745-12-n/</link>
      <pubDate>Sun, 01 Jan 2012 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2012-ap-j-745-12-n/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Census of Star-forming Galaxies at z = 1-3 in the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2011-ap-j-735-91-l/</link>
      <pubDate>Fri, 01 Jul 2011 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2011-ap-j-735-91-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Completing the Census of Ly$α$ Emitters at the Reionization Epoch</title>
      <link>https://astrochun.github.io/publication/2011-ap-j-734-119-k/</link>
      <pubDate>Wed, 01 Jun 2011 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2011-ap-j-734-119-k/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The H$α$ Luminosity Function and Star Formation Rate Volume Density at z = 0.8 from the NEWFIRM H$α$ Survey</title>
      <link>https://astrochun.github.io/publication/2011-ap-j-726-109-l/</link>
      <pubDate>Sat, 01 Jan 2011 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2011-ap-j-726-109-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spitzer Space Telescope Constraint on the Stellar Mass of a z = 6.96 Ly$α$ Emitter</title>
      <link>https://astrochun.github.io/publication/2010-pasj-62-1167-o/</link>
      <pubDate>Fri, 01 Oct 2010 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2010-pasj-62-1167-o/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optical and near-IR spectroscopy of candidate red galaxies in two z ̃ 2.5 proto-clusters</title>
      <link>https://astrochun.github.io/publication/2010-aa-509-a-83-d/</link>
      <pubDate>Fri, 01 Jan 2010 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2010-aa-509-a-83-d/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Radio Imaging of the Very-High-Energy $γ$-Ray Emission Region in the Central Engine of a Radio Galaxy</title>
      <link>https://astrochun.github.io/publication/2009-sci-325-444-a/</link>
      <pubDate>Wed, 01 Jul 2009 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2009-sci-325-444-a/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Search for Molecular Gas toward a BzK-Selected Star-Forming Galaxy at z = 2.044</title>
      <link>https://astrochun.github.io/publication/2009-pasj-61-487-h/</link>
      <pubDate>Mon, 01 Jun 2009 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2009-pasj-61-487-h/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lyman Break Galaxies at z ap 1.8-2.8: GALEX/NUV Imaging of the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2009-ap-j-697-1410-l/</link>
      <pubDate>Mon, 01 Jun 2009 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2009-ap-j-697-1410-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Star Formation Rates and Metallicities of K-Selected Star-Forming Galaxies at z ̃ 2</title>
      <link>https://astrochun.github.io/publication/2009-ap-j-691-140-h/</link>
      <pubDate>Thu, 01 Jan 2009 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2009-ap-j-691-140-h/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Photometric Survey for Ly$α$-He II Dual Emitters: Searching for Population III Stars in High-Redshift Galaxies</title>
      <link>https://astrochun.github.io/publication/2008-ap-j-680-100-n/</link>
      <pubDate>Sun, 01 Jun 2008 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2008-ap-j-680-100-n/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-redshift Ly$α$ emitters with a large equivalent width. Properties of i&#39;-dropout galaxies with an NB921-band depression in the Subaru deep field</title>
      <link>https://astrochun.github.io/publication/2007-aa-468-877-n/</link>
      <pubDate>Fri, 01 Jun 2007 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2007-aa-468-877-n/</guid>
      <description></description>
    </item>
    
    <item>
      <title>High-Frequency VLBI Imaging of the Jet Base of M87</title>
      <link>https://astrochun.github.io/publication/2007-ap-j-660-200-l/</link>
      <pubDate>Tue, 01 May 2007 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2007-ap-j-660-200-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Luminosity Function and Star Formation Rate between Redshifts of 0.07 and 1.47 for Narrowband Emitters in the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2007-ap-j-657-738-l/</link>
      <pubDate>Thu, 01 Mar 2007 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2007-ap-j-657-738-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The End of the Reionization Epoch Probed by Ly$α$ Emitters at z = 6.5 in the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2006-ap-j-648-7-k/</link>
      <pubDate>Fri, 01 Sep 2006 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2006-ap-j-648-7-k/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Ly$α$ Emitters at z = 5.7 in the Subaru Deep Field</title>
      <link>https://astrochun.github.io/publication/2006-pasj-58-313-s/</link>
      <pubDate>Sat, 01 Apr 2006 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2006-pasj-58-313-s/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Discovery of Extended Thermal X-Ray Emission from PKS 2152-699: Evidence for a ``Jet-Cloud&#39;&#39; Interaction</title>
      <link>https://astrochun.github.io/publication/2005-ap-j-618-609-l/</link>
      <pubDate>Sat, 01 Jan 2005 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2005-ap-j-618-609-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Attempt to Probe the Radio Jet Collimation Regions in NGC 4278, NGC 4374 (M84), and NGC 6166</title>
      <link>https://astrochun.github.io/publication/2004-aj-127-119-l/</link>
      <pubDate>Thu, 01 Jan 2004 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2004-aj-127-119-l/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multiband VLA Observations of the Faint Radio Core of 3CR 68.1</title>
      <link>https://astrochun.github.io/publication/2002-aj-124-1943-b/</link>
      <pubDate>Tue, 01 Oct 2002 00:00:00 +0000</pubDate>
      <guid>https://astrochun.github.io/publication/2002-aj-124-1943-b/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
